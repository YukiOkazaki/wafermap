{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提案手法の実験（ラベルが適切か出力）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マルチサイズ\n",
    "- データオーギュメンテーション（鏡映，回転を追加）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import，入力データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/LSWMD.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') memory growth: True\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU') memory growth: True\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LSWMD.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(logical_gpus)\n",
    "import keras\n",
    "from tensorflow.keras import layers, Input, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "# from tf.keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datapath = join('data', 'wafer')\n",
    "print(os.listdir(\"../input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "max_size = 300\n",
    "encord_size = int(max_size / 2)\n",
    "\n",
    "MAKE_DATASET = False\n",
    "TRAIN_AUTO_ENCODER = False\n",
    "\n",
    "cnn_path = './model/cnn_' + str(max_size) + '.h5'\n",
    "\n",
    "epoch = 30\n",
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faulty case list : ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full', 'Random', 'Scratch', 'none']\n"
     ]
    }
   ],
   "source": [
    "faulty_case = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full', 'Random', 'Scratch', 'none']\n",
    "print('Faulty case list : {}'.format(faulty_case))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各ウエハにラベル付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = 2\n",
    "# label_list = []\n",
    "# for i in range(new_x.shape[0]):\n",
    "#     label_dict = {'wafer_id':str(i).zfill(6), 'true_label':y[i][0], 'predict_label':None, 'augmentation':{'noise':0, 'rotation':0, 'inversion':0}, 'trials':trials}\n",
    "#     label_list.append(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(label_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不良ラベルが付いているデータに対してデータオーギュメンテーションを行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習を行う\n",
    "- 不良ラベルを0-8の9次元のベクトルとして表現する．\n",
    "- one-hotエンコーディングを行っている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 保存/読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE_DATASET = False\n",
    "# ver = 'shawon' if shawon else 'propose'\n",
    "# if MAKE_DATASET:\n",
    "#     pickle_dump(new_x, './data/new_x_' + ver + '.pickle')\n",
    "#     pickle_dump(new_y, './data/new_y_' + ver + '.pickle')\n",
    "#     pickle_dump(label_list, './data/label_list_' + ver + '.pickle')\n",
    "    \n",
    "# if not MAKE_DATASET:\n",
    "#     new_x = pickle_load('./data/new_x_' + ver + '.pickle')\n",
    "#     new_y = pickle_load('./data/new_y_' + ver + '.pickle')\n",
    "#     label_list = pickle_load('./data/label_list_' + ver + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, l in enumerate(faulty_case):\n",
    "#     new_y[new_y==l] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(label_list)):\n",
    "#     label_list[i]['true_label'] = new_y[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire the .npy name\n",
    "data_size = len(glob.glob('./data/multi_' + str(max_size) + '/train/' + '*.npy'))\n",
    "TRAINS = ['./data/multi_' + str(max_size) + '/train/' + str(i) + '.npy' for i in range(data_size)]\n",
    "# one-hot-encoding\n",
    "y = joblib.load('./data/multi_' + str(max_size) + '/train/y.pickle')\n",
    "new_y = to_categorical(y)\n",
    "# split test\n",
    "\n",
    "# shuffle_indices = random.sample(list(range(len(TRAINS))), 10000)\n",
    "# TRAINS = [TRAINS[i] for i in shuffle_indices]\n",
    "# new_y = new_y[shuffle_indices]\n",
    "\n",
    "indices = np.array(range(len(TRAINS)))\n",
    "x_train, x_validation, y_train, y_validation, indices_train, indices_validation = train_test_split(\n",
    "    TRAINS, new_y, indices, test_size=0.01, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchを取得する関数\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "def load_array(file):\n",
    "    return np.load(file)\n",
    "\n",
    "def get_batch(batch_size): \n",
    "    global x_train, y_train\n",
    "    SIZE = len(x_train)\n",
    "    # n_batchs\n",
    "    n_batchs = SIZE//batch_size + 1\n",
    "    # for でyield\n",
    "    i = 0\n",
    "    start = time.time()\n",
    "    while (i < n_batchs):\n",
    "        print(\"doing\", i, \"/\", n_batchs)\n",
    "        Y_batch = y_train[(i * batch_size):((i + 1) * batch_size)]\n",
    "        \n",
    "        #あるbatchのfilenameの配列を持っておく\n",
    "        X_batch_name = x_train[(i * batch_size):((i + 1) * batch_size)]\n",
    "\n",
    "        # filenameにしたがってバッチのtensorを構築\n",
    "        with Pool() as p:\n",
    "            arr = p.map(load_array, X_batch_name)\n",
    "            \n",
    "        X_batch = np.array(arr).reshape(len(X_batch_name), max_size, max_size, 3)\n",
    "#         X_batch = np.array([np.load(file)\n",
    "#                             for file in X_batch_name]).reshape(len(X_batch_name), max_size, max_size, 3)\n",
    "        i += 1\n",
    "        print('elapsed time', time.time()-start)\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testsize = 2000\n",
    "# # randlist = rand_ints_nodup(0, new_x.shape[0]-1, testsize)\n",
    "\n",
    "# # valx = new_x.copy()[randlist, :, :, :]\n",
    "# # valy = y.copy()[randlist, :]\n",
    "\n",
    "# test_size = 705 #705\n",
    "# new_x_size = new_x.shape[0]\n",
    "# testlist = rand_ints_nodup(0, new_x_size-1, test_size)\n",
    "# trainlist = [i for i in range(new_x_size) if i != testlist]\n",
    "# new_X=new_x[trainlist]\n",
    "# new_Y=new_y[trainlist]\n",
    "# test_x=new_x[testlist]\n",
    "# test_y=new_y[testlist]\n",
    "\n",
    "# label_train = copy.deepcopy([label_list[i] for i in trainlist])\n",
    "# label_test = copy.deepcopy([label_list[i] for i in testlist])\n",
    "\n",
    "# test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X=new_x\n",
    "# new_Y=new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データとテストデータに分割する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.array(range(new_X.shape[0]))\n",
    "\n",
    "# x_train, x_test, y_train, y_test, indices_train, indices_test = train_test_split(new_X, new_Y, indices,\n",
    "#                                                                 test_size=0.33,\n",
    "#                                                                 random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_vali = copy.deepcopy([label_train[i] for i in indices_test.tolist()])\n",
    "# label_train = copy.deepcopy([label_train[i] for i in indices_train.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x : 299014, y : (299014, 9)\n",
      "Validation x: 3021, y : (3021, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Train x : {}, y : {}'.format(len(x_train), y_train.shape))\n",
    "print('Validation x: {}, y : {}'.format(len(x_validation), y_validation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading X_validation...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading X_validation...\")\n",
    "with Pool() as p:\n",
    "    arr = p.map(load_array, x_validation)\n",
    "\n",
    "x_validation = np.array(arr).reshape(len(x_validation), max_size, max_size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データ246635枚，テストデータ121477枚．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルの定義を行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    with tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"], \n",
    "                                        cross_device_ops = tf.distribute.HierarchicalCopyAllReduce()).scope():\n",
    "        input_shape = (max_size, max_size, 3)\n",
    "        input_tensor = Input(input_shape)\n",
    "\n",
    "        conv_1 = layers.Conv2D(8, (3,3), activation='relu', padding='same')(input_tensor)\n",
    "        conv_2 = layers.Conv2D(16, (3,3), activation='relu', padding='same')(conv_1)\n",
    "        conv_3 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(conv_2)\n",
    "\n",
    "        flat = layers.Flatten()(conv_3)\n",
    "\n",
    "        dense_1 = layers.Dense(64, activation='relu')(flat)\n",
    "        dense_2 = layers.Dense(32, activation='relu')(dense_1)\n",
    "        output_tensor = layers.Dense(9, activation='softmax')(dense_2)\n",
    "\n",
    "        model = models.Model(input_tensor, output_tensor)\n",
    "        model.compile(optimizer='Adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-Fold Cross validationで分割して学習する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=1024, verbose=1) \n",
    "# # 3-Fold Crossvalidation\n",
    "# kfold = KFold(n_splits=3, shuffle=True, random_state=2019) \n",
    "# # results = cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "# # # Check 3-fold model's mean accuracy\n",
    "# # print('Simple CNN Cross validation score : {:.4f}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validiationによる精度は99.55%であった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validationなしで学習する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:XLA_GPU:0')\n",
      "Number of devices: 3\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"device:XLA_GPU:0\"], cross_device_ops = tf.distribute.HierarchicalCopyAllReduce())\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=5\n",
    "batch_size=256\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "==================================================\n",
      "0 / 5\n",
      "doing 0 / 1169\n",
      "elapsed time 0.40367674827575684\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 200.7129 - accuracy: 0.1523\n",
      "batch loss: 200.71286010742188\n",
      "batch accuracy: 0.15234375\n",
      "doing 1 / 1169\n",
      "elapsed time 34.5377094745636\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 284.7155 - accuracy: 0.1016\n",
      "batch loss: 284.7154541015625\n",
      "batch accuracy: 0.1015625\n",
      "doing 2 / 1169\n",
      "elapsed time 58.950785636901855\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 140.6833 - accuracy: 0.1094\n",
      "batch loss: 140.68333435058594\n",
      "batch accuracy: 0.109375\n",
      "doing 3 / 1169\n",
      "elapsed time 85.56406998634338\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 125.4669 - accuracy: 0.1328\n",
      "batch loss: 125.46693420410156\n",
      "batch accuracy: 0.1328125\n",
      "doing 4 / 1169\n",
      "elapsed time 111.98099899291992\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 126.3326 - accuracy: 0.1055\n",
      "batch loss: 126.33264923095703\n",
      "batch accuracy: 0.10546875\n",
      "doing 5 / 1169\n",
      "elapsed time 137.7512936592102\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 72.7114 - accuracy: 0.1250\n",
      "batch loss: 72.7114486694336\n",
      "batch accuracy: 0.125\n",
      "doing 6 / 1169\n",
      "elapsed time 164.0062916278839\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 30.3913 - accuracy: 0.0977\n",
      "batch loss: 30.39129638671875\n",
      "batch accuracy: 0.09765625\n",
      "doing 7 / 1169\n",
      "elapsed time 189.70053887367249\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 13.8354 - accuracy: 0.1602\n",
      "batch loss: 13.835437774658203\n",
      "batch accuracy: 0.16015625\n",
      "doing 8 / 1169\n",
      "elapsed time 215.6559009552002\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 6.7022 - accuracy: 0.1250\n",
      "batch loss: 6.702212810516357\n",
      "batch accuracy: 0.125\n",
      "doing 9 / 1169\n",
      "elapsed time 241.45237684249878\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3.6416 - accuracy: 0.1328\n",
      "batch loss: 3.6416428089141846\n",
      "batch accuracy: 0.1328125\n",
      "doing 10 / 1169\n",
      "elapsed time 267.21478176116943\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.2620 - accuracy: 0.0820\n",
      "batch loss: 2.261981248855591\n",
      "batch accuracy: 0.08203125\n",
      "doing 11 / 1169\n",
      "elapsed time 293.3471472263336\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.2171 - accuracy: 0.1172\n",
      "batch loss: 2.217073440551758\n",
      "batch accuracy: 0.1171875\n",
      "doing 12 / 1169\n",
      "elapsed time 318.7316212654114\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.2185 - accuracy: 0.1016\n",
      "batch loss: 2.2184665203094482\n",
      "batch accuracy: 0.1015625\n",
      "doing 13 / 1169\n",
      "elapsed time 343.3738753795624\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.2026 - accuracy: 0.1016\n",
      "batch loss: 2.2025585174560547\n",
      "batch accuracy: 0.1015625\n",
      "doing 14 / 1169\n",
      "elapsed time 366.89677262306213\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.2066 - accuracy: 0.1094\n",
      "batch loss: 2.2065632343292236\n",
      "batch accuracy: 0.109375\n",
      "doing 15 / 1169\n",
      "elapsed time 393.07198333740234\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1932 - accuracy: 0.0859\n",
      "batch loss: 2.1931753158569336\n",
      "batch accuracy: 0.0859375\n",
      "doing 16 / 1169\n",
      "elapsed time 419.3912971019745\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.2038 - accuracy: 0.1406\n",
      "batch loss: 2.2037696838378906\n",
      "batch accuracy: 0.140625\n",
      "doing 17 / 1169\n",
      "elapsed time 445.6138126850128\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1957 - accuracy: 0.1367\n",
      "batch loss: 2.195690155029297\n",
      "batch accuracy: 0.13671875\n",
      "doing 18 / 1169\n",
      "elapsed time 472.1176857948303\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.2004 - accuracy: 0.1133\n",
      "batch loss: 2.2003583908081055\n",
      "batch accuracy: 0.11328125\n",
      "doing 19 / 1169\n",
      "elapsed time 496.89338636398315\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1937 - accuracy: 0.1602\n",
      "batch loss: 2.193697929382324\n",
      "batch accuracy: 0.16015625\n",
      "doing 20 / 1169\n",
      "elapsed time 520.6358485221863\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1903 - accuracy: 0.1484\n",
      "batch loss: 2.1902806758880615\n",
      "batch accuracy: 0.1484375\n",
      "doing 21 / 1169\n",
      "elapsed time 544.0797243118286\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1891 - accuracy: 0.1289\n",
      "batch loss: 2.189119815826416\n",
      "batch accuracy: 0.12890625\n",
      "doing 22 / 1169\n",
      "elapsed time 568.4786312580109\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1861 - accuracy: 0.1406\n",
      "batch loss: 2.186063289642334\n",
      "batch accuracy: 0.140625\n",
      "doing 23 / 1169\n",
      "elapsed time 593.3854825496674\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1876 - accuracy: 0.1328\n",
      "batch loss: 2.1876306533813477\n",
      "batch accuracy: 0.1328125\n",
      "doing 24 / 1169\n",
      "elapsed time 616.6634924411774\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1908 - accuracy: 0.1016\n",
      "batch loss: 2.1907739639282227\n",
      "batch accuracy: 0.1015625\n",
      "doing 25 / 1169\n",
      "elapsed time 640.7719650268555\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1904 - accuracy: 0.1523\n",
      "batch loss: 2.1904349327087402\n",
      "batch accuracy: 0.15234375\n",
      "doing 26 / 1169\n",
      "elapsed time 666.3963356018066\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1943 - accuracy: 0.1289\n",
      "batch loss: 2.1943018436431885\n",
      "batch accuracy: 0.12890625\n",
      "doing 27 / 1169\n",
      "elapsed time 691.4371144771576\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1941 - accuracy: 0.1172\n",
      "batch loss: 2.1940548419952393\n",
      "batch accuracy: 0.1171875\n",
      "doing 28 / 1169\n",
      "elapsed time 717.0047245025635\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1920 - accuracy: 0.0898\n",
      "batch loss: 2.1920323371887207\n",
      "batch accuracy: 0.08984375\n",
      "doing 29 / 1169\n",
      "elapsed time 741.0839293003082\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1776 - accuracy: 0.1367\n",
      "batch loss: 2.1776299476623535\n",
      "batch accuracy: 0.13671875\n",
      "doing 30 / 1169\n",
      "elapsed time 764.9152529239655\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1911 - accuracy: 0.0898\n",
      "batch loss: 2.1910645961761475\n",
      "batch accuracy: 0.08984375\n",
      "doing 31 / 1169\n",
      "elapsed time 789.0415489673615\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1875 - accuracy: 0.1289\n",
      "batch loss: 2.1874589920043945\n",
      "batch accuracy: 0.12890625\n",
      "doing 32 / 1169\n",
      "elapsed time 814.5055646896362\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1844 - accuracy: 0.1133\n",
      "batch loss: 2.1843886375427246\n",
      "batch accuracy: 0.11328125\n",
      "doing 33 / 1169\n",
      "elapsed time 840.4859087467194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1809 - accuracy: 0.1484\n",
      "batch loss: 2.180910587310791\n",
      "batch accuracy: 0.1484375\n",
      "doing 34 / 1169\n",
      "elapsed time 865.9089155197144\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1875 - accuracy: 0.1133\n",
      "batch loss: 2.187547206878662\n",
      "batch accuracy: 0.11328125\n",
      "doing 35 / 1169\n",
      "elapsed time 891.3984458446503\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1814 - accuracy: 0.1055\n",
      "batch loss: 2.1814332008361816\n",
      "batch accuracy: 0.10546875\n",
      "doing 36 / 1169\n",
      "elapsed time 916.0800135135651\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1567 - accuracy: 0.1523\n",
      "batch loss: 2.1566858291625977\n",
      "batch accuracy: 0.15234375\n",
      "doing 37 / 1169\n",
      "elapsed time 941.3701982498169\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1688 - accuracy: 0.1641\n",
      "batch loss: 2.1687872409820557\n",
      "batch accuracy: 0.1640625\n",
      "doing 38 / 1169\n",
      "elapsed time 965.8117790222168\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1566 - accuracy: 0.1523\n",
      "batch loss: 2.15661883354187\n",
      "batch accuracy: 0.15234375\n",
      "doing 39 / 1169\n",
      "elapsed time 990.3616216182709\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1764 - accuracy: 0.1133\n",
      "batch loss: 2.1763625144958496\n",
      "batch accuracy: 0.11328125\n",
      "doing 40 / 1169\n",
      "elapsed time 1015.9290580749512\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1705 - accuracy: 0.1367\n",
      "batch loss: 2.1704788208007812\n",
      "batch accuracy: 0.13671875\n",
      "doing 41 / 1169\n",
      "elapsed time 1043.900889635086\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1697 - accuracy: 0.1406\n",
      "batch loss: 2.1697068214416504\n",
      "batch accuracy: 0.140625\n",
      "doing 42 / 1169\n",
      "elapsed time 1070.6426229476929\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1458 - accuracy: 0.2031\n",
      "batch loss: 2.145819664001465\n",
      "batch accuracy: 0.203125\n",
      "doing 43 / 1169\n",
      "elapsed time 1095.8956348896027\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1515 - accuracy: 0.1484\n",
      "batch loss: 2.151524066925049\n",
      "batch accuracy: 0.1484375\n",
      "doing 44 / 1169\n",
      "elapsed time 1121.5896265506744\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1300 - accuracy: 0.1367\n",
      "batch loss: 2.1300406455993652\n",
      "batch accuracy: 0.13671875\n",
      "doing 45 / 1169\n",
      "elapsed time 1147.7718620300293\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0585 - accuracy: 0.1758\n",
      "batch loss: 2.0584757328033447\n",
      "batch accuracy: 0.17578125\n",
      "doing 46 / 1169\n",
      "elapsed time 1172.5915553569794\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1480 - accuracy: 0.1172\n",
      "batch loss: 2.1479835510253906\n",
      "batch accuracy: 0.1171875\n",
      "doing 47 / 1169\n",
      "elapsed time 1199.4359922409058\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1374 - accuracy: 0.1602\n",
      "batch loss: 2.1374168395996094\n",
      "batch accuracy: 0.16015625\n",
      "doing 48 / 1169\n",
      "elapsed time 1224.8470964431763\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1186 - accuracy: 0.2344\n",
      "batch loss: 2.1186118125915527\n",
      "batch accuracy: 0.234375\n",
      "doing 49 / 1169\n",
      "elapsed time 1248.5694906711578\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.1010 - accuracy: 0.2969\n",
      "batch loss: 2.101024627685547\n",
      "batch accuracy: 0.296875\n",
      "doing 50 / 1169\n",
      "elapsed time 1272.1444327831268\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 2.1213 - accuracy: 0.2305\n",
      "batch loss: 2.1212706565856934\n",
      "batch accuracy: 0.23046875\n",
      "doing 51 / 1169\n",
      "elapsed time 1297.5566701889038\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.1010 - accuracy: 0.2383\n",
      "batch loss: 2.100983142852783\n",
      "batch accuracy: 0.23828125\n",
      "doing 52 / 1169\n",
      "elapsed time 1322.4179911613464\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0881 - accuracy: 0.2695\n",
      "batch loss: 2.088099956512451\n",
      "batch accuracy: 0.26953125\n",
      "doing 53 / 1169\n",
      "elapsed time 1346.7715468406677\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0746 - accuracy: 0.3125\n",
      "batch loss: 2.074615001678467\n",
      "batch accuracy: 0.3125\n",
      "doing 54 / 1169\n",
      "elapsed time 1370.5644171237946\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0537 - accuracy: 0.3086\n",
      "batch loss: 2.0536892414093018\n",
      "batch accuracy: 0.30859375\n",
      "doing 55 / 1169\n",
      "elapsed time 1394.7135508060455\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.2073 - accuracy: 0.1328\n",
      "batch loss: 2.207329750061035\n",
      "batch accuracy: 0.1328125\n",
      "doing 56 / 1169\n",
      "elapsed time 1419.9621291160583\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0701 - accuracy: 0.1992\n",
      "batch loss: 2.070101499557495\n",
      "batch accuracy: 0.19921875\n",
      "doing 57 / 1169\n",
      "elapsed time 1444.2578766345978\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0346 - accuracy: 0.2305\n",
      "batch loss: 2.034579038619995\n",
      "batch accuracy: 0.23046875\n",
      "doing 58 / 1169\n",
      "elapsed time 1469.7623698711395\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0439 - accuracy: 0.2578\n",
      "batch loss: 2.0438637733459473\n",
      "batch accuracy: 0.2578125\n",
      "doing 59 / 1169\n",
      "elapsed time 1518.7914917469025\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9999 - accuracy: 0.2734\n",
      "batch loss: 1.999875545501709\n",
      "batch accuracy: 0.2734375\n",
      "doing 60 / 1169\n",
      "elapsed time 1568.1566321849823\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0441 - accuracy: 0.2070\n",
      "batch loss: 2.0440564155578613\n",
      "batch accuracy: 0.20703125\n",
      "doing 61 / 1169\n",
      "elapsed time 1618.4270617961884\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0550 - accuracy: 0.2344\n",
      "batch loss: 2.0550448894500732\n",
      "batch accuracy: 0.234375\n",
      "doing 62 / 1169\n",
      "elapsed time 1668.39382314682\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9883 - accuracy: 0.3203\n",
      "batch loss: 1.988344669342041\n",
      "batch accuracy: 0.3203125\n",
      "doing 63 / 1169\n",
      "elapsed time 1717.5878059864044\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9750 - accuracy: 0.2812\n",
      "batch loss: 1.975032091140747\n",
      "batch accuracy: 0.28125\n",
      "doing 64 / 1169\n",
      "elapsed time 1774.4850142002106\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0031 - accuracy: 0.3203\n",
      "batch loss: 2.003145694732666\n",
      "batch accuracy: 0.3203125\n",
      "doing 65 / 1169\n",
      "elapsed time 1825.7084419727325\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0078 - accuracy: 0.2617\n",
      "batch loss: 2.0078487396240234\n",
      "batch accuracy: 0.26171875\n",
      "doing 66 / 1169\n",
      "elapsed time 1880.872373342514\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9781 - accuracy: 0.2578\n",
      "batch loss: 1.9781376123428345\n",
      "batch accuracy: 0.2578125\n",
      "doing 67 / 1169\n",
      "elapsed time 1926.15056848526\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9738 - accuracy: 0.2930\n",
      "batch loss: 1.973766565322876\n",
      "batch accuracy: 0.29296875\n",
      "doing 68 / 1169\n",
      "elapsed time 1957.7790701389313\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0300 - accuracy: 0.2148\n",
      "batch loss: 2.029972553253174\n",
      "batch accuracy: 0.21484375\n",
      "doing 69 / 1169\n",
      "elapsed time 1985.2388844490051\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 2.0502 - accuracy: 0.2617\n",
      "batch loss: 2.0502285957336426\n",
      "batch accuracy: 0.26171875\n",
      "doing 70 / 1169\n",
      "elapsed time 1993.0581600666046\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9607 - accuracy: 0.3008\n",
      "batch loss: 1.9607380628585815\n",
      "batch accuracy: 0.30078125\n",
      "doing 71 / 1169\n",
      "elapsed time 2017.655832529068\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9630 - accuracy: 0.3164\n",
      "batch loss: 1.9630011320114136\n",
      "batch accuracy: 0.31640625\n",
      "doing 72 / 1169\n",
      "elapsed time 2042.1280453205109\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9781 - accuracy: 0.3555\n",
      "batch loss: 1.9780877828598022\n",
      "batch accuracy: 0.35546875\n",
      "doing 73 / 1169\n",
      "elapsed time 2065.4683690071106\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8980 - accuracy: 0.3672\n",
      "batch loss: 1.898010015487671\n",
      "batch accuracy: 0.3671875\n",
      "doing 74 / 1169\n",
      "elapsed time 2089.237169265747\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9495 - accuracy: 0.3438\n",
      "batch loss: 1.9494800567626953\n",
      "batch accuracy: 0.34375\n",
      "doing 75 / 1169\n",
      "elapsed time 2114.4094791412354\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8701 - accuracy: 0.3750\n",
      "batch loss: 1.8700993061065674\n",
      "batch accuracy: 0.375\n",
      "doing 76 / 1169\n",
      "elapsed time 2140.558620929718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9392 - accuracy: 0.3398\n",
      "batch loss: 1.9392063617706299\n",
      "batch accuracy: 0.33984375\n",
      "doing 77 / 1169\n",
      "elapsed time 2164.1038222312927\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9800 - accuracy: 0.2734\n",
      "batch loss: 1.9800477027893066\n",
      "batch accuracy: 0.2734375\n",
      "doing 78 / 1169\n",
      "elapsed time 2188.6889917850494\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9013 - accuracy: 0.3477\n",
      "batch loss: 1.901323914527893\n",
      "batch accuracy: 0.34765625\n",
      "doing 79 / 1169\n",
      "elapsed time 2213.3272585868835\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9253 - accuracy: 0.3477\n",
      "batch loss: 1.9252727031707764\n",
      "batch accuracy: 0.34765625\n",
      "doing 80 / 1169\n",
      "elapsed time 2237.8619182109833\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.9521 - accuracy: 0.2695\n",
      "batch loss: 1.9521002769470215\n",
      "batch accuracy: 0.26953125\n",
      "doing 81 / 1169\n",
      "elapsed time 2263.1764488220215\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8380 - accuracy: 0.2969\n",
      "batch loss: 1.838039517402649\n",
      "batch accuracy: 0.296875\n",
      "doing 82 / 1169\n",
      "elapsed time 2287.951596021652\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9158 - accuracy: 0.2617\n",
      "batch loss: 1.9158449172973633\n",
      "batch accuracy: 0.26171875\n",
      "doing 83 / 1169\n",
      "elapsed time 2312.9568893909454\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8752 - accuracy: 0.2656\n",
      "batch loss: 1.8752050399780273\n",
      "batch accuracy: 0.265625\n",
      "doing 84 / 1169\n",
      "elapsed time 2335.7112181186676\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8496 - accuracy: 0.3125\n",
      "batch loss: 1.8496270179748535\n",
      "batch accuracy: 0.3125\n",
      "doing 85 / 1169\n",
      "elapsed time 2361.4999310970306\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8075 - accuracy: 0.3867\n",
      "batch loss: 1.8074793815612793\n",
      "batch accuracy: 0.38671875\n",
      "doing 86 / 1169\n",
      "elapsed time 2387.2605583667755\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8197 - accuracy: 0.3867\n",
      "batch loss: 1.8197362422943115\n",
      "batch accuracy: 0.38671875\n",
      "doing 87 / 1169\n",
      "elapsed time 2413.156396627426\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8483 - accuracy: 0.3516\n",
      "batch loss: 1.8483312129974365\n",
      "batch accuracy: 0.3515625\n",
      "doing 88 / 1169\n",
      "elapsed time 2438.8987939357758\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8127 - accuracy: 0.4023\n",
      "batch loss: 1.8127481937408447\n",
      "batch accuracy: 0.40234375\n",
      "doing 89 / 1169\n",
      "elapsed time 2465.112417936325\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8838 - accuracy: 0.3516\n",
      "batch loss: 1.8837623596191406\n",
      "batch accuracy: 0.3515625\n",
      "doing 90 / 1169\n",
      "elapsed time 2490.047866821289\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8234 - accuracy: 0.3398\n",
      "batch loss: 1.8233513832092285\n",
      "batch accuracy: 0.33984375\n",
      "doing 91 / 1169\n",
      "elapsed time 2516.2059857845306\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8579 - accuracy: 0.3516\n",
      "batch loss: 1.857861042022705\n",
      "batch accuracy: 0.3515625\n",
      "doing 92 / 1169\n",
      "elapsed time 2543.535621881485\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7547 - accuracy: 0.3984\n",
      "batch loss: 1.754722237586975\n",
      "batch accuracy: 0.3984375\n",
      "doing 93 / 1169\n",
      "elapsed time 2570.92201256752\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7479 - accuracy: 0.3594\n",
      "batch loss: 1.7478994131088257\n",
      "batch accuracy: 0.359375\n",
      "doing 94 / 1169\n",
      "elapsed time 2598.6736719608307\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7761 - accuracy: 0.3750\n",
      "batch loss: 1.7760804891586304\n",
      "batch accuracy: 0.375\n",
      "doing 95 / 1169\n",
      "elapsed time 2624.3809654712677\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8497 - accuracy: 0.3477\n",
      "batch loss: 1.8496763706207275\n",
      "batch accuracy: 0.34765625\n",
      "doing 96 / 1169\n",
      "elapsed time 2652.044167995453\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.7489 - accuracy: 0.3828\n",
      "batch loss: 1.748899221420288\n",
      "batch accuracy: 0.3828125\n",
      "doing 97 / 1169\n",
      "elapsed time 2678.550961256027\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8087 - accuracy: 0.3477\n",
      "batch loss: 1.8087064027786255\n",
      "batch accuracy: 0.34765625\n",
      "doing 98 / 1169\n",
      "elapsed time 2705.3757956027985\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7235 - accuracy: 0.3867\n",
      "batch loss: 1.7235180139541626\n",
      "batch accuracy: 0.38671875\n",
      "doing 99 / 1169\n",
      "elapsed time 2732.3495626449585\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7078 - accuracy: 0.4336\n",
      "batch loss: 1.7077844142913818\n",
      "batch accuracy: 0.43359375\n",
      "doing 100 / 1169\n",
      "elapsed time 2743.6471490859985\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.7663 - accuracy: 0.3906\n",
      "batch loss: 1.7662605047225952\n",
      "batch accuracy: 0.390625\n",
      "doing 101 / 1169\n",
      "elapsed time 2753.2775008678436\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6485 - accuracy: 0.4102\n",
      "batch loss: 1.6484644412994385\n",
      "batch accuracy: 0.41015625\n",
      "doing 102 / 1169\n",
      "elapsed time 2762.1485590934753\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.7785 - accuracy: 0.3711\n",
      "batch loss: 1.7784924507141113\n",
      "batch accuracy: 0.37109375\n",
      "doing 103 / 1169\n",
      "elapsed time 2770.4286375045776\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.7066 - accuracy: 0.4062\n",
      "batch loss: 1.7066123485565186\n",
      "batch accuracy: 0.40625\n",
      "doing 104 / 1169\n",
      "elapsed time 2778.5566313266754\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.8334 - accuracy: 0.4062\n",
      "batch loss: 1.8333585262298584\n",
      "batch accuracy: 0.40625\n",
      "doing 105 / 1169\n",
      "elapsed time 2787.953978061676\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6958 - accuracy: 0.4180\n",
      "batch loss: 1.6957873106002808\n",
      "batch accuracy: 0.41796875\n",
      "doing 106 / 1169\n",
      "elapsed time 2797.2030813694\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.7100 - accuracy: 0.3594\n",
      "batch loss: 1.7099716663360596\n",
      "batch accuracy: 0.359375\n",
      "doing 107 / 1169\n",
      "elapsed time 2805.5885775089264\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6120 - accuracy: 0.4375\n",
      "batch loss: 1.6120350360870361\n",
      "batch accuracy: 0.4375\n",
      "doing 108 / 1169\n",
      "elapsed time 2814.3008482456207\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6871 - accuracy: 0.4141\n",
      "batch loss: 1.6871278285980225\n",
      "batch accuracy: 0.4140625\n",
      "doing 109 / 1169\n",
      "elapsed time 2823.181524038315\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7692 - accuracy: 0.3438\n",
      "batch loss: 1.76922607421875\n",
      "batch accuracy: 0.34375\n",
      "doing 110 / 1169\n",
      "elapsed time 2832.1180045604706\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5451 - accuracy: 0.4570\n",
      "batch loss: 1.5450959205627441\n",
      "batch accuracy: 0.45703125\n",
      "doing 111 / 1169\n",
      "elapsed time 2839.6205875873566\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5654 - accuracy: 0.4180\n",
      "batch loss: 1.5654312372207642\n",
      "batch accuracy: 0.41796875\n",
      "doing 112 / 1169\n",
      "elapsed time 2848.424578666687\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6269 - accuracy: 0.4570\n",
      "batch loss: 1.626857042312622\n",
      "batch accuracy: 0.45703125\n",
      "doing 113 / 1169\n",
      "elapsed time 2857.545912027359\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5397 - accuracy: 0.4570\n",
      "batch loss: 1.539734125137329\n",
      "batch accuracy: 0.45703125\n",
      "doing 114 / 1169\n",
      "elapsed time 2865.9449512958527\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5953 - accuracy: 0.4648\n",
      "batch loss: 1.5953110456466675\n",
      "batch accuracy: 0.46484375\n",
      "doing 115 / 1169\n",
      "elapsed time 2875.633391857147\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6276 - accuracy: 0.4375\n",
      "batch loss: 1.627625823020935\n",
      "batch accuracy: 0.4375\n",
      "doing 116 / 1169\n",
      "elapsed time 2884.5507023334503\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6178 - accuracy: 0.4531\n",
      "batch loss: 1.6177868843078613\n",
      "batch accuracy: 0.453125\n",
      "doing 117 / 1169\n",
      "elapsed time 2893.0430760383606\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5850 - accuracy: 0.4297\n",
      "batch loss: 1.5850093364715576\n",
      "batch accuracy: 0.4296875\n",
      "doing 118 / 1169\n",
      "elapsed time 2901.52223944664\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6875 - accuracy: 0.4102\n",
      "batch loss: 1.6874964237213135\n",
      "batch accuracy: 0.41015625\n",
      "doing 119 / 1169\n",
      "elapsed time 2909.7835812568665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7803 - accuracy: 0.3984\n",
      "batch loss: 1.7803099155426025\n",
      "batch accuracy: 0.3984375\n",
      "doing 120 / 1169\n",
      "elapsed time 2919.517336368561\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4709 - accuracy: 0.4570\n",
      "batch loss: 1.470863699913025\n",
      "batch accuracy: 0.45703125\n",
      "doing 121 / 1169\n",
      "elapsed time 2927.737204313278\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5330 - accuracy: 0.4414\n",
      "batch loss: 1.53303861618042\n",
      "batch accuracy: 0.44140625\n",
      "doing 122 / 1169\n",
      "elapsed time 2936.272951364517\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4964 - accuracy: 0.4688\n",
      "batch loss: 1.4964052438735962\n",
      "batch accuracy: 0.46875\n",
      "doing 123 / 1169\n",
      "elapsed time 2944.5523784160614\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6760 - accuracy: 0.3867\n",
      "batch loss: 1.6759634017944336\n",
      "batch accuracy: 0.38671875\n",
      "doing 124 / 1169\n",
      "elapsed time 2953.590071439743\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5594 - accuracy: 0.4219\n",
      "batch loss: 1.5594098567962646\n",
      "batch accuracy: 0.421875\n",
      "doing 125 / 1169\n",
      "elapsed time 2962.4516940116882\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5110 - accuracy: 0.4648\n",
      "batch loss: 1.5109997987747192\n",
      "batch accuracy: 0.46484375\n",
      "doing 126 / 1169\n",
      "elapsed time 2972.2091398239136\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6005 - accuracy: 0.4180\n",
      "batch loss: 1.6004767417907715\n",
      "batch accuracy: 0.41796875\n",
      "doing 127 / 1169\n",
      "elapsed time 2980.0687367916107\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6184 - accuracy: 0.4453\n",
      "batch loss: 1.6183972358703613\n",
      "batch accuracy: 0.4453125\n",
      "doing 128 / 1169\n",
      "elapsed time 2989.430626630783\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.7115 - accuracy: 0.3867\n",
      "batch loss: 1.7114746570587158\n",
      "batch accuracy: 0.38671875\n",
      "doing 129 / 1169\n",
      "elapsed time 2999.173395395279\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5813 - accuracy: 0.4492\n",
      "batch loss: 1.5813449621200562\n",
      "batch accuracy: 0.44921875\n",
      "doing 130 / 1169\n",
      "elapsed time 3008.243285179138\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5615 - accuracy: 0.4375\n",
      "batch loss: 1.5614955425262451\n",
      "batch accuracy: 0.4375\n",
      "doing 131 / 1169\n",
      "elapsed time 3017.2793061733246\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5871 - accuracy: 0.4609\n",
      "batch loss: 1.5870890617370605\n",
      "batch accuracy: 0.4609375\n",
      "doing 132 / 1169\n",
      "elapsed time 3026.266793012619\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6389 - accuracy: 0.4648\n",
      "batch loss: 1.6389108896255493\n",
      "batch accuracy: 0.46484375\n",
      "doing 133 / 1169\n",
      "elapsed time 3035.900126218796\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6528 - accuracy: 0.4180\n",
      "batch loss: 1.6528024673461914\n",
      "batch accuracy: 0.41796875\n",
      "doing 134 / 1169\n",
      "elapsed time 3045.1973440647125\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5236 - accuracy: 0.5156\n",
      "batch loss: 1.5236464738845825\n",
      "batch accuracy: 0.515625\n",
      "doing 135 / 1169\n",
      "elapsed time 3055.567348718643\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5709 - accuracy: 0.4531\n",
      "batch loss: 1.570876955986023\n",
      "batch accuracy: 0.453125\n",
      "doing 136 / 1169\n",
      "elapsed time 3064.601999282837\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6070 - accuracy: 0.4727\n",
      "batch loss: 1.607006549835205\n",
      "batch accuracy: 0.47265625\n",
      "doing 137 / 1169\n",
      "elapsed time 3073.591877937317\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5282 - accuracy: 0.4414\n",
      "batch loss: 1.5282073020935059\n",
      "batch accuracy: 0.44140625\n",
      "doing 138 / 1169\n",
      "elapsed time 3082.3483238220215\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6614 - accuracy: 0.4219\n",
      "batch loss: 1.6613810062408447\n",
      "batch accuracy: 0.421875\n",
      "doing 139 / 1169\n",
      "elapsed time 3091.9268519878387\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5023 - accuracy: 0.5039\n",
      "batch loss: 1.5023112297058105\n",
      "batch accuracy: 0.50390625\n",
      "doing 140 / 1169\n",
      "elapsed time 3100.4600496292114\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5364 - accuracy: 0.4727\n",
      "batch loss: 1.5364364385604858\n",
      "batch accuracy: 0.47265625\n",
      "doing 141 / 1169\n",
      "elapsed time 3109.2328159809113\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5301 - accuracy: 0.4766\n",
      "batch loss: 1.5300796031951904\n",
      "batch accuracy: 0.4765625\n",
      "doing 142 / 1169\n",
      "elapsed time 3117.747051715851\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5610 - accuracy: 0.4648\n",
      "batch loss: 1.5610026121139526\n",
      "batch accuracy: 0.46484375\n",
      "doing 143 / 1169\n",
      "elapsed time 3126.3411152362823\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4383 - accuracy: 0.5195\n",
      "batch loss: 1.4383041858673096\n",
      "batch accuracy: 0.51953125\n",
      "doing 144 / 1169\n",
      "elapsed time 3134.8598878383636\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5261 - accuracy: 0.4297\n",
      "batch loss: 1.5261447429656982\n",
      "batch accuracy: 0.4296875\n",
      "doing 145 / 1169\n",
      "elapsed time 3143.889172554016\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5435 - accuracy: 0.4297\n",
      "batch loss: 1.5434930324554443\n",
      "batch accuracy: 0.4296875\n",
      "doing 146 / 1169\n",
      "elapsed time 3152.091039419174\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6133 - accuracy: 0.4375\n",
      "batch loss: 1.6133108139038086\n",
      "batch accuracy: 0.4375\n",
      "doing 147 / 1169\n",
      "elapsed time 3160.9937992095947\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5133 - accuracy: 0.4805\n",
      "batch loss: 1.5132715702056885\n",
      "batch accuracy: 0.48046875\n",
      "doing 148 / 1169\n",
      "elapsed time 3169.299563407898\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4283 - accuracy: 0.4766\n",
      "batch loss: 1.4283376932144165\n",
      "batch accuracy: 0.4765625\n",
      "doing 149 / 1169\n",
      "elapsed time 3178.1725659370422\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5310 - accuracy: 0.5078\n",
      "batch loss: 1.5310075283050537\n",
      "batch accuracy: 0.5078125\n",
      "doing 150 / 1169\n",
      "elapsed time 3187.093169927597\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3633 - accuracy: 0.5508\n",
      "batch loss: 1.3633356094360352\n",
      "batch accuracy: 0.55078125\n",
      "doing 151 / 1169\n",
      "elapsed time 3196.1148414611816\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4879 - accuracy: 0.5039\n",
      "batch loss: 1.487879991531372\n",
      "batch accuracy: 0.50390625\n",
      "doing 152 / 1169\n",
      "elapsed time 3205.4999210834503\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4699 - accuracy: 0.5078\n",
      "batch loss: 1.4698803424835205\n",
      "batch accuracy: 0.5078125\n",
      "doing 153 / 1169\n",
      "elapsed time 3214.4373030662537\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4818 - accuracy: 0.4766\n",
      "batch loss: 1.4817699193954468\n",
      "batch accuracy: 0.4765625\n",
      "doing 154 / 1169\n",
      "elapsed time 3223.7619755268097\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4400 - accuracy: 0.4805\n",
      "batch loss: 1.4399514198303223\n",
      "batch accuracy: 0.48046875\n",
      "doing 155 / 1169\n",
      "elapsed time 3232.345965862274\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5036 - accuracy: 0.4609\n",
      "batch loss: 1.5035521984100342\n",
      "batch accuracy: 0.4609375\n",
      "doing 156 / 1169\n",
      "elapsed time 3241.2411744594574\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4566 - accuracy: 0.4609\n",
      "batch loss: 1.4565811157226562\n",
      "batch accuracy: 0.4609375\n",
      "doing 157 / 1169\n",
      "elapsed time 3250.1939516067505\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5673 - accuracy: 0.4453\n",
      "batch loss: 1.5672968626022339\n",
      "batch accuracy: 0.4453125\n",
      "doing 158 / 1169\n",
      "elapsed time 3259.243614912033\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3983 - accuracy: 0.5273\n",
      "batch loss: 1.3983148336410522\n",
      "batch accuracy: 0.52734375\n",
      "doing 159 / 1169\n",
      "elapsed time 3267.684711933136\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4585 - accuracy: 0.5000\n",
      "batch loss: 1.4584803581237793\n",
      "batch accuracy: 0.5\n",
      "doing 160 / 1169\n",
      "elapsed time 3277.3346843719482\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5471 - accuracy: 0.4570\n",
      "batch loss: 1.5470976829528809\n",
      "batch accuracy: 0.45703125\n",
      "doing 161 / 1169\n",
      "elapsed time 3287.056624650955\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4144 - accuracy: 0.4961\n",
      "batch loss: 1.4143664836883545\n",
      "batch accuracy: 0.49609375\n",
      "doing 162 / 1169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 3296.370346546173\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4950 - accuracy: 0.4766\n",
      "batch loss: 1.4950425624847412\n",
      "batch accuracy: 0.4765625\n",
      "doing 163 / 1169\n",
      "elapsed time 3305.4701359272003\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5633 - accuracy: 0.4883\n",
      "batch loss: 1.5633044242858887\n",
      "batch accuracy: 0.48828125\n",
      "doing 164 / 1169\n",
      "elapsed time 3314.37153840065\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5586 - accuracy: 0.4414\n",
      "batch loss: 1.5585546493530273\n",
      "batch accuracy: 0.44140625\n",
      "doing 165 / 1169\n",
      "elapsed time 3323.3207347393036\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4132 - accuracy: 0.5352\n",
      "batch loss: 1.4131584167480469\n",
      "batch accuracy: 0.53515625\n",
      "doing 166 / 1169\n",
      "elapsed time 3332.1881098747253\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4402 - accuracy: 0.4922\n",
      "batch loss: 1.440242052078247\n",
      "batch accuracy: 0.4921875\n",
      "doing 167 / 1169\n",
      "elapsed time 3342.032644033432\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4034 - accuracy: 0.5039\n",
      "batch loss: 1.4034335613250732\n",
      "batch accuracy: 0.50390625\n",
      "doing 168 / 1169\n",
      "elapsed time 3351.1164648532867\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4620 - accuracy: 0.4922\n",
      "batch loss: 1.461991310119629\n",
      "batch accuracy: 0.4921875\n",
      "doing 169 / 1169\n",
      "elapsed time 3359.023943424225\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4835 - accuracy: 0.4883\n",
      "batch loss: 1.4834630489349365\n",
      "batch accuracy: 0.48828125\n",
      "doing 170 / 1169\n",
      "elapsed time 3367.892388343811\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.6125 - accuracy: 0.4766\n",
      "batch loss: 1.6125004291534424\n",
      "batch accuracy: 0.4765625\n",
      "doing 171 / 1169\n",
      "elapsed time 3376.4772334098816\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4558 - accuracy: 0.5195\n",
      "batch loss: 1.4558172225952148\n",
      "batch accuracy: 0.51953125\n",
      "doing 172 / 1169\n",
      "elapsed time 3385.1148161888123\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4436 - accuracy: 0.5117\n",
      "batch loss: 1.4436036348342896\n",
      "batch accuracy: 0.51171875\n",
      "doing 173 / 1169\n",
      "elapsed time 3393.9643013477325\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3852 - accuracy: 0.4883\n",
      "batch loss: 1.3852332830429077\n",
      "batch accuracy: 0.48828125\n",
      "doing 174 / 1169\n",
      "elapsed time 3403.0380659103394\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3668 - accuracy: 0.5586\n",
      "batch loss: 1.366774559020996\n",
      "batch accuracy: 0.55859375\n",
      "doing 175 / 1169\n",
      "elapsed time 3412.3058700561523\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4329 - accuracy: 0.5117\n",
      "batch loss: 1.432941198348999\n",
      "batch accuracy: 0.51171875\n",
      "doing 176 / 1169\n",
      "elapsed time 3422.3517742156982\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4257 - accuracy: 0.5156\n",
      "batch loss: 1.4256796836853027\n",
      "batch accuracy: 0.515625\n",
      "doing 177 / 1169\n",
      "elapsed time 3430.7387330532074\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4487 - accuracy: 0.5117\n",
      "batch loss: 1.4486510753631592\n",
      "batch accuracy: 0.51171875\n",
      "doing 178 / 1169\n",
      "elapsed time 3439.771091938019\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.5198 - accuracy: 0.4688\n",
      "batch loss: 1.5197710990905762\n",
      "batch accuracy: 0.46875\n",
      "doing 179 / 1169\n",
      "elapsed time 3448.587720155716\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.6276 - accuracy: 0.4531\n",
      "batch loss: 1.6276159286499023\n",
      "batch accuracy: 0.453125\n",
      "doing 180 / 1169\n",
      "elapsed time 3457.604965209961\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3986 - accuracy: 0.4922\n",
      "batch loss: 1.3985862731933594\n",
      "batch accuracy: 0.4921875\n",
      "doing 181 / 1169\n",
      "elapsed time 3466.8538525104523\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4005 - accuracy: 0.5195\n",
      "batch loss: 1.400464653968811\n",
      "batch accuracy: 0.51953125\n",
      "doing 182 / 1169\n",
      "elapsed time 3476.6900568008423\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4755 - accuracy: 0.5000\n",
      "batch loss: 1.4754846096038818\n",
      "batch accuracy: 0.5\n",
      "doing 183 / 1169\n",
      "elapsed time 3487.0406301021576\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3886 - accuracy: 0.5195\n",
      "batch loss: 1.3885602951049805\n",
      "batch accuracy: 0.51953125\n",
      "doing 184 / 1169\n",
      "elapsed time 3495.460874557495\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3702 - accuracy: 0.5352\n",
      "batch loss: 1.3702268600463867\n",
      "batch accuracy: 0.53515625\n",
      "doing 185 / 1169\n",
      "elapsed time 3504.299387693405\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3214 - accuracy: 0.5703\n",
      "batch loss: 1.3213986158370972\n",
      "batch accuracy: 0.5703125\n",
      "doing 186 / 1169\n",
      "elapsed time 3512.3481669425964\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3334 - accuracy: 0.5625\n",
      "batch loss: 1.3333876132965088\n",
      "batch accuracy: 0.5625\n",
      "doing 187 / 1169\n",
      "elapsed time 3521.051648378372\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3696 - accuracy: 0.5508\n",
      "batch loss: 1.3696043491363525\n",
      "batch accuracy: 0.55078125\n",
      "doing 188 / 1169\n",
      "elapsed time 3529.042269229889\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4401 - accuracy: 0.5234\n",
      "batch loss: 1.4401350021362305\n",
      "batch accuracy: 0.5234375\n",
      "doing 189 / 1169\n",
      "elapsed time 3536.9294509887695\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2914 - accuracy: 0.5703\n",
      "batch loss: 1.2914096117019653\n",
      "batch accuracy: 0.5703125\n",
      "doing 190 / 1169\n",
      "elapsed time 3546.925174474716\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2723 - accuracy: 0.5742\n",
      "batch loss: 1.2722587585449219\n",
      "batch accuracy: 0.57421875\n",
      "doing 191 / 1169\n",
      "elapsed time 3555.572376728058\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3914 - accuracy: 0.5117\n",
      "batch loss: 1.3914090394973755\n",
      "batch accuracy: 0.51171875\n",
      "doing 192 / 1169\n",
      "elapsed time 3564.3910250663757\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4561 - accuracy: 0.4922\n",
      "batch loss: 1.4561045169830322\n",
      "batch accuracy: 0.4921875\n",
      "doing 193 / 1169\n",
      "elapsed time 3572.833237171173\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.4452 - accuracy: 0.5273\n",
      "batch loss: 1.4452143907546997\n",
      "batch accuracy: 0.52734375\n",
      "doing 194 / 1169\n",
      "elapsed time 3581.640800714493\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3949 - accuracy: 0.5195\n",
      "batch loss: 1.3948884010314941\n",
      "batch accuracy: 0.51953125\n",
      "doing 195 / 1169\n",
      "elapsed time 3590.2096202373505\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3717 - accuracy: 0.5625\n",
      "batch loss: 1.3717455863952637\n",
      "batch accuracy: 0.5625\n",
      "doing 196 / 1169\n",
      "elapsed time 3598.9776496887207\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2560 - accuracy: 0.6133\n",
      "batch loss: 1.2559888362884521\n",
      "batch accuracy: 0.61328125\n",
      "doing 197 / 1169\n",
      "elapsed time 3607.412571668625\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3642 - accuracy: 0.5664\n",
      "batch loss: 1.364167332649231\n",
      "batch accuracy: 0.56640625\n",
      "doing 198 / 1169\n",
      "elapsed time 3616.2402522563934\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.4551 - accuracy: 0.4961\n",
      "batch loss: 1.4551348686218262\n",
      "batch accuracy: 0.49609375\n",
      "doing 199 / 1169\n",
      "elapsed time 3625.1566393375397\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3669 - accuracy: 0.5781\n",
      "batch loss: 1.3668758869171143\n",
      "batch accuracy: 0.578125\n",
      "doing 200 / 1169\n",
      "elapsed time 3634.344220161438\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3144 - accuracy: 0.5508\n",
      "batch loss: 1.3144302368164062\n",
      "batch accuracy: 0.55078125\n",
      "doing 201 / 1169\n",
      "elapsed time 3643.1788799762726\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2737 - accuracy: 0.5742\n",
      "batch loss: 1.2736637592315674\n",
      "batch accuracy: 0.57421875\n",
      "doing 202 / 1169\n",
      "elapsed time 3651.733596086502\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2366 - accuracy: 0.5703\n",
      "batch loss: 1.23664128780365\n",
      "batch accuracy: 0.5703125\n",
      "doing 203 / 1169\n",
      "elapsed time 3659.939644098282\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3144 - accuracy: 0.5508\n",
      "batch loss: 1.3144199848175049\n",
      "batch accuracy: 0.55078125\n",
      "doing 204 / 1169\n",
      "elapsed time 3668.0826301574707\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3051 - accuracy: 0.5664\n",
      "batch loss: 1.305090308189392\n",
      "batch accuracy: 0.56640625\n",
      "doing 205 / 1169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 3677.085534095764\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3005 - accuracy: 0.5586\n",
      "batch loss: 1.3004775047302246\n",
      "batch accuracy: 0.55859375\n",
      "doing 206 / 1169\n",
      "elapsed time 3687.020094871521\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2766 - accuracy: 0.5664\n",
      "batch loss: 1.2766220569610596\n",
      "batch accuracy: 0.56640625\n",
      "doing 207 / 1169\n",
      "elapsed time 3696.0073478221893\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3338 - accuracy: 0.5820\n",
      "batch loss: 1.333793044090271\n",
      "batch accuracy: 0.58203125\n",
      "doing 208 / 1169\n",
      "elapsed time 3704.634191274643\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.3957 - accuracy: 0.5312\n",
      "batch loss: 1.3956512212753296\n",
      "batch accuracy: 0.53125\n",
      "doing 209 / 1169\n",
      "elapsed time 3713.1375319957733\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2657 - accuracy: 0.6133\n",
      "batch loss: 1.2657263278961182\n",
      "batch accuracy: 0.61328125\n",
      "doing 210 / 1169\n",
      "elapsed time 3721.3248727321625\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2968 - accuracy: 0.5859\n",
      "batch loss: 1.296755313873291\n",
      "batch accuracy: 0.5859375\n",
      "doing 211 / 1169\n",
      "elapsed time 3729.3603382110596\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2237 - accuracy: 0.5820\n",
      "batch loss: 1.2237234115600586\n",
      "batch accuracy: 0.58203125\n",
      "doing 212 / 1169\n",
      "elapsed time 3739.426248550415\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3771 - accuracy: 0.5508\n",
      "batch loss: 1.3770558834075928\n",
      "batch accuracy: 0.55078125\n",
      "doing 213 / 1169\n",
      "elapsed time 3748.0800762176514\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2285 - accuracy: 0.5820\n",
      "batch loss: 1.2284977436065674\n",
      "batch accuracy: 0.58203125\n",
      "doing 214 / 1169\n",
      "elapsed time 3756.4388434886932\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2248 - accuracy: 0.6055\n",
      "batch loss: 1.2247846126556396\n",
      "batch accuracy: 0.60546875\n",
      "doing 215 / 1169\n",
      "elapsed time 3765.0788037776947\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2602 - accuracy: 0.5859\n",
      "batch loss: 1.2602031230926514\n",
      "batch accuracy: 0.5859375\n",
      "doing 216 / 1169\n",
      "elapsed time 3773.726038455963\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1243 - accuracy: 0.5898\n",
      "batch loss: 1.1243243217468262\n",
      "batch accuracy: 0.58984375\n",
      "doing 217 / 1169\n",
      "elapsed time 3782.2198507785797\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0881 - accuracy: 0.6602\n",
      "batch loss: 1.0881097316741943\n",
      "batch accuracy: 0.66015625\n",
      "doing 218 / 1169\n",
      "elapsed time 3790.7725121974945\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1320 - accuracy: 0.6211\n",
      "batch loss: 1.132042407989502\n",
      "batch accuracy: 0.62109375\n",
      "doing 219 / 1169\n",
      "elapsed time 3799.2796874046326\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2290 - accuracy: 0.5430\n",
      "batch loss: 1.2290005683898926\n",
      "batch accuracy: 0.54296875\n",
      "doing 220 / 1169\n",
      "elapsed time 3808.275969028473\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3137 - accuracy: 0.5469\n",
      "batch loss: 1.3137094974517822\n",
      "batch accuracy: 0.546875\n",
      "doing 221 / 1169\n",
      "elapsed time 3816.7044780254364\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2366 - accuracy: 0.5859\n",
      "batch loss: 1.2366244792938232\n",
      "batch accuracy: 0.5859375\n",
      "doing 222 / 1169\n",
      "elapsed time 3825.9142775535583\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2468 - accuracy: 0.5898\n",
      "batch loss: 1.246810793876648\n",
      "batch accuracy: 0.58984375\n",
      "doing 223 / 1169\n",
      "elapsed time 3835.303201198578\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1327 - accuracy: 0.6406\n",
      "batch loss: 1.132713794708252\n",
      "batch accuracy: 0.640625\n",
      "doing 224 / 1169\n",
      "elapsed time 3844.0280718803406\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1669 - accuracy: 0.6133\n",
      "batch loss: 1.166938066482544\n",
      "batch accuracy: 0.61328125\n",
      "doing 225 / 1169\n",
      "elapsed time 3852.4992396831512\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1460 - accuracy: 0.6133\n",
      "batch loss: 1.1460471153259277\n",
      "batch accuracy: 0.61328125\n",
      "doing 226 / 1169\n",
      "elapsed time 3861.122871160507\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2721 - accuracy: 0.5547\n",
      "batch loss: 1.2721223831176758\n",
      "batch accuracy: 0.5546875\n",
      "doing 227 / 1169\n",
      "elapsed time 3869.995465993881\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2809 - accuracy: 0.5664\n",
      "batch loss: 1.2808955907821655\n",
      "batch accuracy: 0.56640625\n",
      "doing 228 / 1169\n",
      "elapsed time 3879.5016140937805\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1463 - accuracy: 0.6211\n",
      "batch loss: 1.146275520324707\n",
      "batch accuracy: 0.62109375\n",
      "doing 229 / 1169\n",
      "elapsed time 3888.013808965683\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0557 - accuracy: 0.6641\n",
      "batch loss: 1.0556713342666626\n",
      "batch accuracy: 0.6640625\n",
      "doing 230 / 1169\n",
      "elapsed time 3896.3696835041046\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1288 - accuracy: 0.6172\n",
      "batch loss: 1.1288237571716309\n",
      "batch accuracy: 0.6171875\n",
      "doing 231 / 1169\n",
      "elapsed time 3904.656352043152\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2065 - accuracy: 0.6211\n",
      "batch loss: 1.2065255641937256\n",
      "batch accuracy: 0.62109375\n",
      "doing 232 / 1169\n",
      "elapsed time 3913.0192873477936\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1802 - accuracy: 0.6289\n",
      "batch loss: 1.1802031993865967\n",
      "batch accuracy: 0.62890625\n",
      "doing 233 / 1169\n",
      "elapsed time 3921.367513179779\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2963 - accuracy: 0.5703\n",
      "batch loss: 1.296274185180664\n",
      "batch accuracy: 0.5703125\n",
      "doing 234 / 1169\n",
      "elapsed time 3929.9685130119324\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0916 - accuracy: 0.6172\n",
      "batch loss: 1.091644048690796\n",
      "batch accuracy: 0.6171875\n",
      "doing 235 / 1169\n",
      "elapsed time 3938.0213198661804\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2257 - accuracy: 0.5703\n",
      "batch loss: 1.2256882190704346\n",
      "batch accuracy: 0.5703125\n",
      "doing 236 / 1169\n",
      "elapsed time 3946.7346074581146\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1205 - accuracy: 0.6406\n",
      "batch loss: 1.1204859018325806\n",
      "batch accuracy: 0.640625\n",
      "doing 237 / 1169\n",
      "elapsed time 3955.007299184799\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0482 - accuracy: 0.6602\n",
      "batch loss: 1.048181414604187\n",
      "batch accuracy: 0.66015625\n",
      "doing 238 / 1169\n",
      "elapsed time 3963.0582613945007\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1855 - accuracy: 0.5820\n",
      "batch loss: 1.1855227947235107\n",
      "batch accuracy: 0.58203125\n",
      "doing 239 / 1169\n",
      "elapsed time 3972.0751128196716\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1956 - accuracy: 0.6133\n",
      "batch loss: 1.1955921649932861\n",
      "batch accuracy: 0.61328125\n",
      "doing 240 / 1169\n",
      "elapsed time 3980.613871574402\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0957 - accuracy: 0.6445\n",
      "batch loss: 1.0956873893737793\n",
      "batch accuracy: 0.64453125\n",
      "doing 241 / 1169\n",
      "elapsed time 3989.449727535248\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2311 - accuracy: 0.6055\n",
      "batch loss: 1.2311227321624756\n",
      "batch accuracy: 0.60546875\n",
      "doing 242 / 1169\n",
      "elapsed time 3997.227928876877\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1538 - accuracy: 0.5938\n",
      "batch loss: 1.153825044631958\n",
      "batch accuracy: 0.59375\n",
      "doing 243 / 1169\n",
      "elapsed time 4005.6160728931427\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0245 - accuracy: 0.6719\n",
      "batch loss: 1.024533987045288\n",
      "batch accuracy: 0.671875\n",
      "doing 244 / 1169\n",
      "elapsed time 4014.001041173935\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1389 - accuracy: 0.5859\n",
      "batch loss: 1.1389057636260986\n",
      "batch accuracy: 0.5859375\n",
      "doing 245 / 1169\n",
      "elapsed time 4022.56871676445\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0824 - accuracy: 0.6211\n",
      "batch loss: 1.0824110507965088\n",
      "batch accuracy: 0.62109375\n",
      "doing 246 / 1169\n",
      "elapsed time 4031.4170825481415\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0789 - accuracy: 0.6523\n",
      "batch loss: 1.0789375305175781\n",
      "batch accuracy: 0.65234375\n",
      "doing 247 / 1169\n",
      "elapsed time 4039.9139988422394\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1246 - accuracy: 0.6562\n",
      "batch loss: 1.1245918273925781\n",
      "batch accuracy: 0.65625\n",
      "doing 248 / 1169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 4049.424317598343\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1419 - accuracy: 0.6289\n",
      "batch loss: 1.1418870687484741\n",
      "batch accuracy: 0.62890625\n",
      "doing 249 / 1169\n",
      "elapsed time 4057.6547949314117\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1410 - accuracy: 0.6328\n",
      "batch loss: 1.1409518718719482\n",
      "batch accuracy: 0.6328125\n",
      "doing 250 / 1169\n",
      "elapsed time 4065.9125785827637\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1714 - accuracy: 0.6016\n",
      "batch loss: 1.171410322189331\n",
      "batch accuracy: 0.6015625\n",
      "doing 251 / 1169\n",
      "elapsed time 4074.0183877944946\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.2006 - accuracy: 0.5547\n",
      "batch loss: 1.2005503177642822\n",
      "batch accuracy: 0.5546875\n",
      "doing 252 / 1169\n",
      "elapsed time 4082.042383670807\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0272 - accuracy: 0.6172\n",
      "batch loss: 1.0271589756011963\n",
      "batch accuracy: 0.6171875\n",
      "doing 253 / 1169\n",
      "elapsed time 4091.8052928447723\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0226 - accuracy: 0.6562\n",
      "batch loss: 1.0226459503173828\n",
      "batch accuracy: 0.65625\n",
      "doing 254 / 1169\n",
      "elapsed time 4100.1316010952\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0905 - accuracy: 0.6211\n",
      "batch loss: 1.090511679649353\n",
      "batch accuracy: 0.62109375\n",
      "doing 255 / 1169\n",
      "elapsed time 4109.065726518631\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0710 - accuracy: 0.6211\n",
      "batch loss: 1.071016550064087\n",
      "batch accuracy: 0.62109375\n",
      "doing 256 / 1169\n",
      "elapsed time 4118.032475233078\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1686 - accuracy: 0.6055\n",
      "batch loss: 1.168626070022583\n",
      "batch accuracy: 0.60546875\n",
      "doing 257 / 1169\n",
      "elapsed time 4126.894796133041\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.1433 - accuracy: 0.6367\n",
      "batch loss: 1.143284797668457\n",
      "batch accuracy: 0.63671875\n",
      "doing 258 / 1169\n",
      "elapsed time 4135.313126087189\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.2657 - accuracy: 0.5508\n",
      "batch loss: 1.2656886577606201\n",
      "batch accuracy: 0.55078125\n",
      "doing 259 / 1169\n",
      "elapsed time 4144.536878824234\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.0958 - accuracy: 0.6328\n",
      "batch loss: 1.0957534313201904\n",
      "batch accuracy: 0.6328125\n",
      "doing 260 / 1169\n",
      "elapsed time 4153.2804272174835\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.1456 - accuracy: 0.6289\n",
      "batch loss: 1.145557165145874\n",
      "batch accuracy: 0.62890625\n",
      "doing 261 / 1169\n",
      "elapsed time 4161.790022611618\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.0585 - accuracy: 0.6055\n",
      "batch loss: 1.0584566593170166\n",
      "batch accuracy: 0.60546875\n",
      "doing 262 / 1169\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_validation_loss = []\n",
    "epoch_validation_acc = []\n",
    "for ep in range(epoch):\n",
    "    print(\"=\" * 50)\n",
    "    print(ep, \"/\", epoch)\n",
    "    step_loss = []\n",
    "    step_acc = []\n",
    "    \n",
    "    # batch_size=1000でHDDからバッチを取得する\n",
    "    for X_batch, Y_batch in get_batch(batch_size):\n",
    "        model.train_on_batch(X_batch, Y_batch)\n",
    "        score = model.evaluate(X_batch, Y_batch)\n",
    "        print(\"batch loss:\", score[0])\n",
    "        print(\"batch accuracy:\", score[1])\n",
    "        step_loss.append(score[0])\n",
    "        step_acc.append(score[1])\n",
    "    print(\"Train loss\", np.mean(step_loss))\n",
    "    print(\"Train accuracy\", np.mean(step_acc))\n",
    "    score = model.evaluate(x_validation, y_validation)\n",
    "    print(\"Validation loss:\", score[0])\n",
    "    print(\"Validation accuracy:\", score[1])\n",
    "    epoch_train_loss.append(np.mean(step_loss))\n",
    "    epoch_train_acc.append(np.mean(step_acc))\n",
    "    epoch_validation_loss.append(score[0])\n",
    "    epoch_validation_acc.append(score[1])\n",
    "    \n",
    "    shuffle_indices = random.sample(list(range(len(x_train))), len(x_train))\n",
    "    x_train = [x_train[i] for i in shuffle_indices]\n",
    "    y_train = y_train[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\"):\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストデータで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = joblib.load('./data/multi_' + str(max_size) + '/test/xtest.pickle')\n",
    "y_test = joblib.load('./data/multi_' + str(max_size) + '/test/ytest.pickle')\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "testscore = score[1]\n",
    "trainscore = epoch_train_acc[-1]\n",
    "valiscore = epoch_validation_acc[-1]\n",
    "print(\"Train accuracy:\", trainscore)\n",
    "print(\"Validation accuracy:\", valiscore)\n",
    "print(\"Test accuracy:\", testscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.argmax(model.predict(x_test), axis=1)\n",
    "y_test_max = np.argmax(y_test, axis=1)\n",
    "np.sum(y_test_max == y_predict, axis=0, dtype='float') / x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルは以下．\n",
    "    - 入力層\n",
    "    - 畳み込み層3つ\n",
    "    - Flatten層（1次元に）\n",
    "    - 全結合層3つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accuracyグラフ，lossグラフは以下．\n",
    "- 5epoch程度で落ち着いている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy plot\n",
    "fig1 = plt.figure()\n",
    "plt.plot(epoch_train_acc)\n",
    "plt.plot(epoch_validation_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig1.savefig(\"multisize_accuracy.png\")\n",
    "\n",
    "# loss plot\n",
    "fig2 = plt.figure()\n",
    "plt.plot(epoch_train_loss)\n",
    "plt.plot(epoch_validation_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig2.savefig(\"multisize_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    fmt = '.4f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.xticks([0, 1, 2, 3, 4, 5, 6, 7 ,8], [\"Center\", \"Donut\", \"Edge-Loc\", \"Edge-Ring\", \"Loc\", \"Near-full\", \"Random\", \"Scratch\", \"None\"])\n",
    "    plt.yticks([0, 1, 2, 3, 4, 5, 6, 7 ,8], [\"Center\", \"Donut\", \"Edge-Loc\", \"Edge-Ring\", \"Loc\", \"Near-full\", \"Random\", \"Scratch\", \"None\"])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- validation confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "y_validation_predict = np.argmax(model.predict(x_validation), axis=1)\n",
    "y_validation_max = np.argmax(y_validation, axis=1)\n",
    "cnf_matrix = confusion_matrix(y_validation_max, y_validation_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "from matplotlib import gridspec\n",
    "fig = plt.figure(figsize=(8, 15)) \n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1]) \n",
    "\n",
    "## Plot non-normalized confusion matrix\n",
    "plt.subplot(gs[0])\n",
    "plot_confusion_matrix(cnf_matrix, title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(gs[1])\n",
    "plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"multisize_valiconfmat.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "y_test_predict = np.argmax(model.predict(x_test), axis=1)\n",
    "y_test_max = np.argmax(y_test, axis=1)\n",
    "cnf_matrix = confusion_matrix(y_test_max, y_test_predict)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "from matplotlib import gridspec\n",
    "fig = plt.figure(figsize=(8, 15)) \n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1]) \n",
    "\n",
    "## Plot non-normalized confusion matrix\n",
    "plt.subplot(gs[0])\n",
    "plot_confusion_matrix(cnf_matrix, title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.subplot(gs[1])\n",
    "plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"multisize_testconfmat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var notebook_name = document.body.attributes['data-notebook-name'].value\n",
    "IPython.notebook.kernel.execute(`NOTEBOOK_NAME = '${notebook_name}'`);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# LINEの設定\n",
    "path = './lineapi.txt'\n",
    "with open(path) as f:\n",
    "    s = f.read()\n",
    "    line_token = s.rstrip('\\n')\n",
    "\n",
    "# LINEに通知する関数\n",
    "def line_notify(text):\n",
    "    url = \"https://notify-api.line.me/api/notify\"\n",
    "    data = {\"message\": text}\n",
    "    headers = {\"Authorization\": \"Bearer \" + line_token}\n",
    "    proxies = {\n",
    "        'http': 'http://proxy.uec.ac.jp:8080',\n",
    "        'https': 'https://proxy.uec.ac.jp:8080',\n",
    "    }\n",
    "    requests.post(url, data=data, headers=headers)#, proxies=proxies)\n",
    "\n",
    "# LINEに画像を送る関数\n",
    "def line_notify_img(text, imgpath):\n",
    "    url = \"https://notify-api.line.me/api/notify\"\n",
    "    data = {\"message\": text, \"notificationDisabled\": True}\n",
    "    files = {\"imageFile\": open(imgpath, \"rb\")}\n",
    "    headers = {\"Authorization\": \"Bearer \" + line_token}\n",
    "    proxies = {\n",
    "        'http': 'http://proxy.uec.ac.jp:8080',\n",
    "        'https': 'https://proxy.uec.ac.jp:8080',\n",
    "    }\n",
    "    requests.post(url, data=data, files=files, headers=headers)#, proxies=proxies)\n",
    "    \n",
    "line_notify(\"学習が終了しました \" + NOTEBOOK_NAME)\n",
    "# line_notify(\"Shawon: \" + str(shawon) + \", rotation_num: \" + str(rotation_num) + \", inversion: \" + str(inversion) + \", trials: \" + str(trials))\n",
    "line_notify_img(\"正解率\", \"multisize_accuracy.png\")\n",
    "line_notify_img(\"Loss\", \"multisize_loss.png\")\n",
    "line_notify_img(\"validation混同行列\", \"multisize_valiconfmat.png\")\n",
    "line_notify_img(\"test混同行列\", \"multisize_testconfmat.png\")\n",
    "line_notify(\"train:\" + str(trainscore) + \"\\nvali:\" + str(valiscore) + \"\\ntest:\" + str(testscore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wafermap",
   "language": "python",
   "name": "wafermap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
