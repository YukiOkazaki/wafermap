{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ウエハサイズを限定せずに機械学習させる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import，入力データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/LSWMD.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LSWMD.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, Input, models\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datapath = join('data', 'wafer')\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MAKE_DATASET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    df=pd.read_pickle(\"../input/LSWMD.pkl\")\n",
    "\n",
    "    df = df.drop(['waferIndex'], axis = 1)\n",
    "\n",
    "    def find_dim(x):\n",
    "        dim0=np.size(x,axis=0)\n",
    "        dim1=np.size(x,axis=1)\n",
    "        return dim0,dim1\n",
    "    df['waferMapDim']=df.waferMap.apply(find_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    df['failureNum']=df.failureType\n",
    "    df['trainTestNum']=df.trianTestLabel\n",
    "    mapping_type={'Center':0,'Donut':1,'Edge-Loc':2,'Edge-Ring':3,'Loc':4,'Random':5,'Scratch':6,'Near-full':7,'none':8}\n",
    "    mapping_traintest={'Training':0,'Test':1}\n",
    "    df=df.replace({'failureNum':mapping_type, 'trainTestNum':mapping_traintest})\n",
    "\n",
    "    tol_wafers = df.shape[0]\n",
    "\n",
    "    df_withlabel = df[(df['failureNum']>=0) & (df['failureNum']<=8)]\n",
    "    df_withlabel =df_withlabel.reset_index()\n",
    "    df_withpattern = df[(df['failureNum']>=0) & (df['failureNum']<=7)]\n",
    "    df_withpattern = df_withpattern.reset_index()\n",
    "    df_nonpattern = df[(df['failureNum']==8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データサイズ関係なく処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使えるデータサイズを求める\n",
    "    - None以外の合計が50個以上のウエハ\n",
    "    - サイズが100以下\n",
    "    - 統一サイズを100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    uni_waferDim=np.unique(df.waferMapDim, return_counts=True)\n",
    "    wdim = uni_waferDim[0]\n",
    "    failure_list = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Random', 'Scratch', 'Near-full', 'none']\n",
    "    usable_wdim_list = []\n",
    "    usable_wafer_num = 0\n",
    "    max_size = 100\n",
    "    for i in range(len(wdim)):\n",
    "        sub_df = df.loc[df['waferMapDim'] == wdim[i]]\n",
    "        pattern_num = [0] * 9\n",
    "        for j in range(len(sub_df)):\n",
    "            if len(sub_df.iloc[j,:]['failureType']) == 0:\n",
    "                continue\n",
    "            pattern = sub_df.iloc[j,:]['failureType'][0][0]\n",
    "            pattern_num[failure_list.index(pattern)] += 1\n",
    "        if sum(pattern_num) - pattern_num[8] >= 50 and wdim[i][0] <= max_size and wdim[i][1] <= max_size:\n",
    "            usable_wdim_list.append(wdim[i])\n",
    "            print(wdim[i], len(sub_df), sum(pattern_num))\n",
    "            usable_wafer_num += sum(pattern_num)\n",
    "    print(usable_wafer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unisize_wafer(size, wafer):\n",
    "    width, height = wafer.shape\n",
    "    unisize_wafer = np.zeros((size, size))\n",
    "    width_pad = int((size - width) / 2)\n",
    "    height_pad = int((size - height) / 2)\n",
    "    unisize_wafer[width_pad:width_pad + width, height_pad:height_pad + height] = wafer\n",
    "    return unisize_wafer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    sw = np.ones((usable_wafer_num, max_size, max_size), dtype='int8')\n",
    "    label = list()\n",
    "    count = 0\n",
    "    for usable_wdim in usable_wdim_list:\n",
    "        sub_df = df.loc[df['waferMapDim'] == usable_wdim]\n",
    "        sub_wafer = sub_df['waferMap'].values\n",
    "        print(usable_wdim)\n",
    "        print(len(sub_df))\n",
    "\n",
    "        for i in range(len(sub_df)):\n",
    "            # skip null label\n",
    "            if len(sub_df.iloc[i,:]['failureType']) == 0:\n",
    "                continue\n",
    "            sw[count] = make_unisize_wafer(max_size, sub_df.iloc[i,:]['waferMap'])\n",
    "            label.append(sub_df.iloc[i,:]['failureType'][0][0])\n",
    "            count += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(\" \", i)\n",
    "    x = sw\n",
    "    y = np.array(label).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xとyをファイルに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    faulty_case = np.unique(y)\n",
    "    print('Faulty case list : {}'.format(faulty_case))\n",
    "if not MAKE_DATASET:\n",
    "    faulty_case = ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full', 'Random', 'Scratch', 'none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    for f in faulty_case :\n",
    "        print('{} : {}'.format(f, len(y[y==f])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    for i, l in enumerate(faulty_case):\n",
    "        y[y==l] = int(i)\n",
    "        print(type(i))\n",
    "    y = y.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj,f,protocol=4)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "if MAKE_DATASET:\n",
    "#     joblib.dump(x, './data/xmulti.pickle')\n",
    "    joblib.dump(y, './data/ymulti.pickle')\n",
    "    \n",
    "if not MAKE_DATASET:\n",
    "#     x = joblib.load('./data/xmulti.pickle')\n",
    "    y = joblib.load('./data/ymulti.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    print('x shape : {}, y shape : {}'.format(x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 4025\n",
      "1 : 474\n",
      "2 : 4512\n",
      "3 : 8819\n",
      "4 : 3000\n",
      "5 : 139\n",
      "6 : 760\n",
      "7 : 918\n",
      "8 : 133934\n"
     ]
    }
   ],
   "source": [
    "for i in range(9) :\n",
    "    print('{} : {}'.format(i, len(y[y==i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最初のデータを可視化してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    # plot 1st data\n",
    "    plt.imshow(x[0,:, :, 0])\n",
    "    plt.show()\n",
    "\n",
    "    # check faulty case\n",
    "    print('Faulty case : {} '.format(faulty_case[y[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    x = x.reshape((-1, 100, 100, 1))\n",
    "    x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 14366枚の26x26ウエハの不良パターンは上記のようになっている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    new_x = np.zeros((len(x), 100, 100, 3), dtype='int8')\n",
    "\n",
    "    for w in range(len(x)):\n",
    "        for i in range(100):\n",
    "            for j in range(100):\n",
    "                new_x[w, i, j, int(x[w, i, j])] = 1\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DATASET:\n",
    "    joblib.dump(new_x, './data/new_xmulti.pickle')\n",
    "    \n",
    "if not MAKE_DATASET:\n",
    "    new_x = joblib.load('./data/new_xmulti.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- new_xを(14366, 26, 26, 3)とし，最後の次元にはウエハの値(0, 1, 2)がそれぞれの値毎にベクトルとしてまとめられている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n",
      "|       ImageDataGenerator|      1064|\n",
      "|                       In|       272|\n",
      "|                    Input|       144|\n",
      "|                    KFold|      1064|\n",
      "|          KerasClassifier|      1064|\n",
      "|             MAKE_DATASET|        24|\n",
      "|                      Out|       248|\n",
      "|                     copy|        88|\n",
      "|          cross_val_score|       144|\n",
      "|                      csv|        88|\n",
      "|                      cv2|        88|\n",
      "|                 datapath|        59|\n",
      "|                  dirname|        57|\n",
      "|                     exit|        64|\n",
      "|              faulty_case|       144|\n",
      "|                 filename|        58|\n",
      "|                filenames|       104|\n",
      "|              get_ipython|        72|\n",
      "|                        i|        28|\n",
      "|                    image|        88|\n",
      "|                   joblib|        88|\n",
      "|                     join|       144|\n",
      "|                    keras|        88|\n",
      "|                   layers|        88|\n",
      "|       make_unisize_wafer|       144|\n",
      "|                   models|        88|\n",
      "|                    new_x|4697430144|\n",
      "|                       np|        88|\n",
      "|                       os|        88|\n",
      "|                       pd|        88|\n",
      "|                   pickle|        88|\n",
      "|              pickle_dump|       144|\n",
      "|              pickle_load|       144|\n",
      "|                      plt|        88|\n",
      "|                     quit|        64|\n",
      "|                      sys|        88|\n",
      "|                       tf|        88|\n",
      "|           to_categorical|       144|\n",
      "|         train_test_split|       144|\n",
      "|                 warnings|        88|\n",
      "|                        y|    156693|\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\"):\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オートエンコーダで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### エンコーダとデコーダのモデルを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルの定義をする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_shape = (100, 100, 3)\n",
    "input_tensor = Input(input_shape)\n",
    "encode = layers.Conv2D(64, (3,3), padding='same', activation='relu')(input_tensor)\n",
    "\n",
    "latent_vector = layers.MaxPool2D()(encode)\n",
    "\n",
    "# Decoder\n",
    "decode_layer_1 = layers.Conv2DTranspose(64, (3,3), padding='same', activation='relu')\n",
    "decode_layer_2 = layers.UpSampling2D()\n",
    "output_tensor = layers.Conv2DTranspose(3, (3,3), padding='same', activation='sigmoid')\n",
    "\n",
    "# connect decoder layers\n",
    "decode = decode_layer_1(latent_vector)\n",
    "decode = decode_layer_2(decode)\n",
    "\n",
    "ae = models.Model(input_tensor, output_tensor(decode))\n",
    "ae.compile(optimizer = 'Adam',\n",
    "              loss = 'mse',\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 100, 100, 3)       1731      \n",
      "=================================================================\n",
      "Total params: 40,451\n",
      "Trainable params: 40,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 層は\n",
    "    - 入力層\n",
    "    - 畳み込み層\n",
    "    - プーリング層\n",
    "    - 転置畳み込み層\n",
    "    - アップサンプリング層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=3\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習を開始する．\n",
    "- `new_x`を`new_x`にエンコードしデコードする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "156581/156581 [==============================] - 79s 508us/step - loss: 0.0028\n",
      "Epoch 2/3\n",
      "156581/156581 [==============================] - 79s 502us/step - loss: 1.6953e-04\n",
      "Epoch 3/3\n",
      "156581/156581 [==============================] - 79s 504us/step - loss: 9.4820e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0750024fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start train\n",
    "ae.fit(new_x, new_x,\n",
    "       batch_size=batch_size,\n",
    "       epochs=epoch,\n",
    "       verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- エンコーダだけのモデルを定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Model(input_tensor, latent_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- デコーダだけのモデルを定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input((50, 50, 64))\n",
    "decode = decode_layer_1(decoder_input)\n",
    "decode = decode_layer_2(decode)\n",
    "\n",
    "decoder = models.Model(decoder_input, output_tensor(decode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `encoder`を使って元のウエハ画像をエンコードする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode original faulty wafer\n",
    "# encoded_x = np.zeros((156581, 50, 50, 64), dtype=\"int8\")\n",
    "# encoded_x = (encoder.predict(new_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- エンコードされた潜伏的な不良ウエハにノイズを負荷する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to encoded latent faulty wafers vector.\n",
    "# noised_encoded_x = encoded_x + np.random.normal(loc=0, scale=0.1, size = (len(encoded_x), 50, 50, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 元のウエハ画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original faulty wafer data\n",
    "# plt.imshow(np.argmax(new_x[3], axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ノイズが付加されたウエハ画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check new noised faulty wafer data\n",
    "# noised_gen_x = np.argmax(decoder.predict(noised_encoded_x), axis=3)\n",
    "# plt.imshow(noised_gen_x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データオーギュメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データオーギュメンテーションを行う関数を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment function define\n",
    "def gen_data(wafer, label):\n",
    "    # Encode input wafer\n",
    "    encoded_x = encoder.predict(wafer)\n",
    "    \n",
    "    # dummy array for collecting noised wafer\n",
    "    gen_x = np.zeros((1, 100, 100, 3), dtype='int8')\n",
    "    \n",
    "    # Make wafer until total # of wafer to 2000\n",
    "    for i in range((20000//len(encoded_x)) + 1):\n",
    "        noised_encoded_x = encoded_x + np.random.normal(loc=0, scale=0.1, size = (len(encoded_x), 50, 50, 64)) \n",
    "        noised_gen_x = decoder.predict(noised_encoded_x)\n",
    "        gen_x = np.concatenate((gen_x, noised_gen_x), axis=0)\n",
    "    # also make label vector with same length\n",
    "    gen_y = np.full((len(gen_x), 1), label)\n",
    "    \n",
    "    print(label, gen_x.shape)\n",
    "    \n",
    "    # return date without 1st dummy data.\n",
    "    return gen_x[1:], gen_y[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不良ラベルが付いているデータに対してデータオーギュメンテーションを行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_idx = np.where(y==8)[0][np.random.choice(len(np.where(y==8)[0]), size=117000, replace=False)]\n",
    "new_x = np.delete(new_x, none_idx, axis=0)\n",
    "y = np.delete(y, none_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation for all faulty case.\n",
    "for i in range(9) : \n",
    "    # skip none case\n",
    "    if i == 8 : \n",
    "        continue\n",
    "    \n",
    "    gen_x, gen_y = gen_data(new_x[np.where(y==i)[0]], i)\n",
    "    print(\"gen\")\n",
    "    new_x = np.concatenate((new_x, gen_x), axis=0)\n",
    "    print(\"x\")\n",
    "    y = np.concatenate((y, gen_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After Generate new_x shape : {}, new_y shape : {}'.format(new_x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9) :\n",
    "    print('{} : {}'.format(i, len(y[y==i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データオーギュメンテーションを行った結果，各不良データごとに2000枚増えた．\n",
    "- 合計は30707枚となった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不良ラベルのないデータは削除し，枚数を不良ラベルと同程度にする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_idx = np.where(y==8)[0][np.random.choice(len(np.where(y==8)[0]), size=120000, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.delete(new_x, none_idx, axis=0)\n",
    "new_y = np.delete(y, none_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After Delete \"none\" class new_x shape : {}, new_y shape : {}'.format(new_x.shape, new_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9) :\n",
    "    print('{} : {}'.format(i, len(new_y[new_y==i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 削除した結果，全体は19707枚となった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習を行う\n",
    "- 不良ラベルを0-8の9次元のベクトルとして表現する．\n",
    "- one-hotエンコーディングを行っている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, l in enumerate(faulty_case):\n",
    "#     new_y[new_y==l] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding\n",
    "new_y = to_categorical(new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データ（学習データと学習時のテストデータ）と最終的なテストデータに分割する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X=new_x[0:19000]\n",
    "# new_Y=new_y[0:19000]\n",
    "# test_x=new_x[19001:19706]\n",
    "# test_y=new_y[19001:19706]\n",
    "# test_x.shape\n",
    "new_X = new_x\n",
    "new_Y = new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データを学習データと学習時のテストデータに分割する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new_X, new_Y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train x : {}, y : {}'.format(x_train.shape, y_train.shape))\n",
    "print('Test x: {}, y : {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 学習データ12730枚，テストデータ6270枚．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルの定義を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_shape = (100, 100, 3)\n",
    "    input_tensor = Input(input_shape)\n",
    "\n",
    "    conv_1 = layers.Conv2D(16, (3,3), activation='relu', padding='same')(input_tensor)\n",
    "    conv_2 = layers.Conv2D(32, (3,3), activation='relu', padding='same')(conv_1)\n",
    "    conv_3 = layers.Conv2D(64, (3,3), activation='relu', padding='same')(conv_2)\n",
    "\n",
    "    flat = layers.Flatten()(conv_3)\n",
    "\n",
    "    dense_1 = layers.Dense(256, activation='relu')(flat)\n",
    "    dense_2 = layers.Dense(64, activation='relu')(dense_1)\n",
    "    output_tensor = layers.Dense(9, activation='softmax')(dense_2)\n",
    "\n",
    "    model = models.Model(input_tensor, output_tensor)\n",
    "    model.compile(optimizer='Adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-Fold Cross validationで分割して学習する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=1, verbose=1) \n",
    "# 3-Fold Crossvalidation\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=2019) \n",
    "#results = cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "# Check 3-fold model's mean accuracy\n",
    "#print('Simple CNN Cross validation score : {:.4f}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validiationによる精度は99.10%であった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validationなしで学習する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del new_x\n",
    "# del new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=30\n",
    "batch_size=256\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "         validation_data=[x_test, y_test],\n",
    "         epochs=epoch,\n",
    "         batch_size=batch_size,\n",
    "         verbose=1           \n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- テストデータで評価．    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "#print('Test Loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "print('Testing Accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- acuurayは99.31%であった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- モデルは以下．\n",
    "    - 入力層\n",
    "    - 畳み込み層3つ\n",
    "    - Flatten層（1次元に）\n",
    "    - 全結合層3つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accuracyグラフ，lossグラフは以下．\n",
    "- 5epoch程度で落ち着いている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy plot \n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
